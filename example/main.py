import os
import streamlit as st
import base64
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import ChatMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores.faiss import FAISS
from langserve import RemoteRunnable
from langchain.schema import Document
import fitz  # PyMuPDF


RAG_PROMPT_TEMPLATE = """
ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:
{context}

ì§ˆë¬¸:
{question}

ì§ˆë¬¸ì˜ í•µì‹¬ë§Œ íŒŒì•…í•˜ì—¬ ê°„ê²°í•˜ê²Œ 1-2ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ë¶ˆí•„ìš”í•œ ì„¤ëª…ì€ í”¼í•˜ë©° ë™ì„œìš¸ëŒ€í•™êµì™€ ê´€ë ¨ëœ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”.

ë‹µë³€:
"""

st.set_page_config(page_title="Chatbot PY", page_icon="ğŸ’¬")
st.title("DU Chatbot íŒŒì´")

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="DU Chatbot íŒŒì´ì…ë‹ˆë‹¤.")
    ]

# ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©í•˜ëŠ” í•¨ìˆ˜
def get_base64_of_bin_file(bin_file):
    with open(bin_file, "rb") as f:
        data = f.read()
    return base64.b64encode(data).decode()

# ë°±ê·¸ë¼ìš´ë“œ ì´ë¯¸ì§€ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜
def set_bg_hack(main_bg):
    bin_str = get_base64_of_bin_file(main_bg)
    page_bg_img = f"""
    <style>
    [data-testid="stAppViewContainer"]::before {{
        content: "";
        background-image: url("data:image/png;base64,{bin_str}");
        background-size: 30%;
        background-position: center;
        background-repeat: no-repeat;
        background-attachment: fixed;
        opacity: 0.1;
        position: absolute;
        top: 0;
        left: 0;
        bottom: 0;
        right: 0;
    }}
    [data-testid="stHeader"] {{
        background-color: rgba(0,0,0,0);
    }}
    [data-testid="stToolbar"] {{
        right: 2rem;
    }}
    </style>
    """
    st.markdown(page_bg_img, unsafe_allow_html=True)

def print_history():
    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)
        
def add_history(role, content):
    st.session_state.messages.append(ChatMessage(role=role, content=content))

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

set_bg_hack("./DU_logo.png")

# RAG êµ¬í˜„
doc = fitz.open("./QADataset.pdf")
text = ""
for page in doc:
    text += page.get_text()

splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=50)
chunk_temp = splitter.split_text(text)
chunks = [Document(page_content=t) for t in chunk_temp]

# device ì„¤ì •ì„ CPU ë˜ëŠ” CUDAë¡œ ë³€ê²½
model_kwargs = {"device": "cuda"} 
encode_kwargs = {"normalize_embeddings": True}
embeddings = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-large",
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs,
)

db = FAISS.from_documents(chunks, embedding=embeddings)
retriever = db.as_retriever(search_kwargs={"k": 2})

st.sidebar.title("ì±„íŒ… ëª©ë¡")
option = st.sidebar.selectbox(
    "ì±„íŒ…ëª©ë¡ì„ ì„ íƒí•˜ì„¸ìš”.:", ("ë™ì„œìš¸ëŒ€í•™êµ íœ´í•™ ì‹ ì²­", "ë…¼ë¬¸ ìš”ì²­", "ì¥í•™ìƒ ì¤€ë¹„")
)

st.write(f"{option}")

if st.sidebar.button("BJ.Park"):
    st.sidebar.text("ìº¡ìŠ¤í†¤ ë””ìì¸ ì¬ë°Œì—ˆìŠµë‹ˆë‹¤.")
if st.sidebar.button("J.Jeong"):
    st.sidebar.text("ëª¨ë“  ê°œë°œì€ ì €ë¥¼ í†µí•©ë‹ˆë‹¤.")
if st.sidebar.button("JH.Lee"):
    st.sidebar.text("ì£¼í¬ is free.")
if st.sidebar.button("SH.Kim"):
    st.sidebar.text("ê¹€ìŠ¹í˜„ì„ êµ­íšŒë¡œ!!!!!!. ")

print_history()

if user_input := st.chat_input():
    add_history("user", user_input)
    st.chat_message("user").write(user_input)
    with st.chat_message("assistant"):
        ollama = RemoteRunnable("ngrok server url/llm/")
        chat_container = st.empty()
        prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

        rag_chain = (
            {
                "context": retriever | format_docs,
                "question": RunnablePassthrough(),
            }
            | prompt
            | ollama
            | StrOutputParser()
        )
        answer = rag_chain.stream(user_input)
        chunks = []
        for chunk in answer:
            chunks.append(chunk)
            chat_container.markdown("".join(chunks))
        add_history("ai", "".join(chunks))
